return {
  'huggingface/llm.nvim',
  opts = {
    backend = 'ollama',
    url = "http://localhost:11434",
    model = 'phi3'
  }
}
